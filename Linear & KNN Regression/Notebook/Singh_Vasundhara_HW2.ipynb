{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Singh_Vasundhara_HW2</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Vasundhara Singh\n",
    "<br>\n",
    "Github Username: vasu1406\n",
    "<br>\n",
    "USC ID: 7421414643"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Combined Cycle Power Plant Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import statsmodels.api as statsModel\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor as KClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Cycle Power Plant Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../Data/CCPP/Folds5x2_pp.xlsx');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 9568 rows and 5 columns. \n",
    "The first 4 columns AT = Average temperature, V= Exhaust vacuum, AP= Ambient Pressure, RH=Relatively Humidity are the \n",
    "independent variables\n",
    "The dependent variable PE = energy output. The rows represent the data/observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of Rows =\", len(df))\n",
    "print(\"No. of Columns =\",len(df.columns))\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. pairwise scatterplots of all the varianbles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the scatterplot below, the independent variable 'PE' seems to have some kind of linear relationship with all the dependent variables. We can also find some relations amongst the dependent variables as well. For example AT and V, and some weak relationship between AT and RH. AT has most linear relationship relationship with PE as seen in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. mean, the median, range, first and third quartiles, and interquartile ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prettytable\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "summary = PrettyTable([\"Column Name\", \"Mean\", \"Median\", \"Range\", \"First Quartile\", \"Third Quartile\",\"Interquartile Ranges\"])\n",
    "  \n",
    "# Add rows\n",
    "for column in df.columns:\n",
    "    thirdQ=df[column].quantile(0.75)\n",
    "    firstQ=df[column].quantile(0.25)\n",
    "    summary.add_row([column,round(df[column].mean(),6),round(df[column].median(),6),df[column].quantile(1)-df[column].quantile(0),firstQ,thirdQ,thirdQ-firstQ])\n",
    "  \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using Cook's Distance to find out the outliers in the data. Here we identify the outliers by the data which is more than 4/n in the cooks distance. These outliers are shown in Red color in the plots. As we can see there are significant no. of outliers in each regression task, we can remove them to get better prediction.\n",
    "\n",
    "All the independent variables have a statistically significant association with the dependent variable as all of them have a\n",
    "p-value much less than 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fetch_cooks_points(cooks,x,y):\n",
    "    outliersMap = dict()\n",
    "    n=len(cooks)\n",
    "    \n",
    "    for i in range(0,n):\n",
    "        if cooks[i] >= 4/n:\n",
    "            outliersMap[x[i]]=y[i]\n",
    "    return outliersMap\n",
    "\n",
    "coeffMap=dict()\n",
    "independentVariables = df.columns[:-1]\n",
    "dependentVariables = df.columns[-1]\n",
    "energyOutputColumn = df['PE']\n",
    "\n",
    "for column in independentVariables:\n",
    "    \n",
    "    model = statsModel.OLS(energyOutputColumn,statsModel.add_constant(df[column])).fit()\n",
    "    sns.regplot(x = column,y = dependentVariables, data = df,line_kws = {'color': 'black'})\n",
    "\n",
    "    cooks = model.get_influence().cooks_distance\n",
    "    \n",
    "    \n",
    "    cooks_map = fetch_cooks_points(cooks[0],df[column],df['PE'])\n",
    "    keys = cooks_map.keys()\n",
    "    values = cooks_map.values()\n",
    "\n",
    "#     print(cooks_map)\n",
    "    plt.scatter(keys,values,color ='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print(model.params)\n",
    "    coeffMap[column] = [column,model.params[column]]\n",
    "    results_summary = model.summary()\n",
    "#     p_values = results_summary.tables[1]['P>|t|']\n",
    "#     table = results_summary.tables[1]\n",
    "#     table = pd.DataFrame(table.data)\n",
    "#     p_values = table.loc[:, table.columns == 'P>|t|'].iloc[:, 0].tolist()\n",
    "#     print('P Value',p_values)\n",
    "    print(results_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis states that the independent variable has null or no effect on the dependent variable. But as we see in the results below, even after fitting a multiple regression model to predict the response, the p values are much less than 0.05 and\n",
    "way closer to 0, which shows all the predictors have a statistically significant association with the dependent variable. Hence\n",
    "we can reject the null hypothesis for all the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independentVariables = df.columns[:-1]\n",
    "energyOutputColumn = df['PE']\n",
    "\n",
    "multipleRegressionModel = statsModel.OLS(energyOutputColumn,statsModel.add_constant(df[independentVariables])).fit()\n",
    "results_summary_multiple = multipleRegressionModel.summary()\n",
    "\n",
    "print(results_summary_multiple)\n",
    "print(energyOutputColumn.shape)\n",
    "print(df[independentVariables].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) 1c Compare to 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1c results shows that each independent variable have statistically significant association with the dependent variable and 1d \n",
    "results shows that there is statistical significance when combined set of all the independent variables are used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipleRegressionModel.params\n",
    "for column in independentVariables:\n",
    "    coeffMap[column].append(multipleRegressionModel.params[column])\n",
    "for keys,values in coeffMap.items():\n",
    "    print(keys,' ',values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map = pd.DataFrame.from_dict(coeffMap,orient='index',columns=['Attributes','Univariate Coefficients','Multivariate Coefficients'])\n",
    "\n",
    "mapRows=[]\n",
    "columnNames=['Attributes','Univariate Coefficients','Multivariate Coefficients']\n",
    "for key,values in coeffMap.items():\n",
    "    mapRows.append([key,values[1],values[2]])\n",
    "\n",
    "pd.DataFrame(mapRows,columns = columnNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Univariate Coefficients',y='Multivariate Coefficients',data = map,hue='Attributes')\n",
    "plt.title(\"Univariate VS Multivariate Coefficients\")\n",
    "plt.ylabel('Multivariate Coefficients')\n",
    "plt.xlabel('Univariate Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Nonlinear Association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some predictors have statistical significance when they have squared or cubic terms. For example- AT,AP and RH has statistical significance in linear,quadratic and cubic term as the p-value is close to 0. It's good idea to explore their non linear relationship with the dependent variable as p-values is very low, hence they have a non-linear relationship with the dependant variable. However, V is statistically insignificant when squared, as V^2 has very high p-value. Hence V^2 doesn't have a strong non-linear relationship with the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in independentVariables:\n",
    "    polynomial = PolynomialFeatures(3)\n",
    "    polynomial_wrt_x = pd.DataFrame(polynomial.fit_transform(df[col].values.reshape(-1,1)),columns = polynomial.get_feature_names_out([col]))\n",
    "    model = statsModel.OLS(endog = energyOutputColumn,exog = polynomial_wrt_x).fit()\n",
    "    print('Attribute =',col)\n",
    "    display(model.summary())\n",
    "#     tables = model.summary().tables[1]\n",
    "#     print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) Interactions of Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As AT X V, AT X RH, V X AP, AP X RH have very small p values which are less than 0.05. All these interaction terms have statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interaction set to true\n",
    "\n",
    "polynomial_Interactions = PolynomialFeatures(2, interaction_only = True)\n",
    "x_Interactions = df[independentVariables]\n",
    "polynomial_wrt_all_terms = pd.DataFrame(polynomial_Interactions.fit_transform(x_Interactions),columns = polynomial_Interactions.get_feature_names_out(independentVariables))\n",
    "model = statsModel.OLS(energyOutputColumn,polynomial_wrt_all_terms).fit()\n",
    "display(model.summary())\n",
    "# tables = model.summary().tables[1]\n",
    "# print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (h) Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_wrt_x=PolynomialFeatures(2)\n",
    "polynomial = PolynomialFeatures(2)\n",
    "x_Interactions = df[independentVariables]\n",
    "xData = pd.DataFrame(polynomial.fit_transform(x_Interactions),columns=polynomial.get_feature_names_out(independentVariables))\n",
    "trainingX, testX, trainingY, testY = tts(xData, energyOutputColumn, test_size = 0.3, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_col = ['1'] + [col for col in independentVariables]\n",
    "\n",
    "model = statsModel.OLS(trainingY,trainingX[arr_col]).fit()\n",
    "predictedYTest = model.predict(testX[arr_col])\n",
    "predictedYTrain = model.predict(trainingX[arr_col])\n",
    "\n",
    "minSquaredErrorTest = mean_squared_error(testY,predictedYTest)\n",
    "minSquaredErrorTrain = mean_squared_error(trainingY,predictedYTrain)\n",
    "print('Min Squared Error Test =',minSquaredErrorTest)\n",
    "print('Min Squared Error Train =',minSquaredErrorTrain)\n",
    "\n",
    "display(model.summary())\n",
    "# tables = model.summary().tables[1]\n",
    "# display(tables)\n",
    "# print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that V^2, V X AP, and V X RH have very high p-values hence they are statistically insignificant, hence I have dropped it and calculated the MSE. Removing these terms increases the MSE very slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All terms and interaction_Terms\n",
    "\n",
    "model = statsModel.OLS(trainingY,trainingX).fit()\n",
    "predictedYTest = model.predict(testX)\n",
    "predictedYTrain = model.predict(trainingX)\n",
    "\n",
    "minSquaredErrorTest = mean_squared_error(testY,predictedYTest)\n",
    "minSquaredErrorTrain = mean_squared_error(trainingY,predictedYTrain)\n",
    "print('Min Squared Error Test =',minSquaredErrorTest)\n",
    "print('Min Squared Error Train =',minSquaredErrorTrain)\n",
    "\n",
    "display(model.summary())\n",
    "# tables = model.summary().tables[1]\n",
    "# print(tables)\n",
    "\n",
    "print('\\n\\n')\n",
    "print('After removing statistically insignificant terms, the MSE increases slightly\\n\\n')\n",
    "\n",
    "# Remove all the statistically insignificant terms (V^2, VXRH and VXAP) which have p values greater than 0.05\n",
    "# As we are removing some of terms, the MSE increases slightly\n",
    "\n",
    "significantX=['1','AT','V','AP','RH','AT^2','AT V','AT RH','AP^2','AP RH','RH^2']\n",
    "\n",
    "model = statsModel.OLS(trainingY,trainingX[significantX]).fit()\n",
    "predictedYTest = model.predict(testX[significantX])\n",
    "predictedYTrain = model.predict(trainingX[significantX])\n",
    "\n",
    "\n",
    "minSquaredErrorTest = mean_squared_error(testY,predictedYTest)\n",
    "minSquaredErrorTrain = mean_squared_error(trainingY,predictedYTrain)\n",
    "print('Min Squared Error Test =',minSquaredErrorTest)\n",
    "print('Min Squared Error Train =',minSquaredErrorTrain)\n",
    "\n",
    "display(model.summary())\n",
    "# tables = model.summary().tables[1]\n",
    "# print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k value for least error is around 4-5. It changes sometimes because of random slicing of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnKeywithMinimumError(errorMap):\n",
    "    return min(zip(errorMap.values(), errorMap.keys()))[1]\n",
    "\n",
    "def KNearestNeighbors(trainingX,trainingY,testX,testY):\n",
    "    \n",
    "    errorMapTrain = dict()\n",
    "    errorMapTest = dict()\n",
    "    \n",
    "    for k in range(1,101):\n",
    "        \n",
    "        kRegression = KClassifier(n_neighbors = k)\n",
    "        kRegression.fit(trainingX, trainingY)\n",
    "        \n",
    "        predictedYTest = kRegression.predict(testX)\n",
    "        predictedYTrain = kRegression.predict(trainingX)\n",
    "        \n",
    "        errorMapTest[1/k] = mean_squared_error(testY,predictedYTest)\n",
    "        errorMapTrain[1/k] = mean_squared_error(trainingY,predictedYTrain)\n",
    "    \n",
    "    optimalK = returnKeywithMinimumError(errorMapTest)\n",
    "    leastError = errorMapTest[optimalK]\n",
    "    \n",
    "    print('Most Optimal K=',1/optimalK,' with least error = ',leastError)\n",
    "    \n",
    "    # Plot train and test errors in terms of k\n",
    "\n",
    "    plt.plot(*zip(*sorted(errorMapTrain.items())), label='Training Error')\n",
    "    plt.plot(*zip(*sorted(errorMapTest.items())), label = 'Test Error')\n",
    "    plt.xlabel('1/k')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print('\\n\\nRAW DATA')\n",
    "KNearestNeighbors(trainingX[independentVariables],trainingY, testX[independentVariables], testY)\n",
    "\n",
    "#Normalisation\n",
    "# trainingData = trainingX[independentVariables]\n",
    "# testData = testX[independentVariables]\n",
    "scaler = MinMaxScaler()\n",
    "trainingXNormalized = pd.DataFrame(scaler.fit_transform(trainingX))\n",
    "testXNormalized = pd.DataFrame(scaler.fit_transform(testX))\n",
    "\n",
    "\n",
    "print('\\n\\nNORMALISED DATA')\n",
    "KNearestNeighbors(trainingXNormalized,trainingY,testXNormalized,testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (j ) Compare KNN and Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min Squared Error Test = 18.64731247858754 for Multiple Linear Regression Model with all interaction terms and non-linear terms.\n",
    "\n",
    "FOR KNN \n",
    "RAW DATA- Most Optimal K= 4.0-5.0  with least error approximately 15.726819842563568\n",
    "NORMALISED DATA\n",
    "Most Optimal K= 8.0-9.0  with least error approximately 16.023740527581857\n",
    "\n",
    "\n",
    "The results clearly shows that the Minimum Squared Error for KNN Regression is lesser than the Minimum Squared Error obtained in\n",
    "different Linear Regression Models. Hence KNN is better suited for the data. And in case of our data, normalisation didn't work \n",
    "in favor of us and the MSE was increased.\n",
    "\n",
    "In KNN, it is hard to interpret the effect of each term on the output. On the \n",
    "other hand we can understand the impact of each attribute on the output throught the coefficients we get in Linear Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ISLR: 2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) The sample size n is extremely large, and the number of predictors p is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a flexible statistical learning method would have better performance than an inflexible one. As the n is large, and hence the sample size is large and we would have enough information to model the relationships between predictors and output\n",
    "better, even if there is a complex model. In addition to that, having a small no. of predictors means the model will not have \n",
    "'overfitting' or the curse of dimensionality which can make the performance of inflexible methods suffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) The number of predictors p is extremely large, and the number of observations n is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, an inflexible statistical learning method would have better performance than a flexible one. As the n is small,\n",
    "and hence the sample size is small, we would not have enough information to model the relationships between predictors and\n",
    "output/response accurately. If the no. of predictors is high, it makes it even more difficult to develop an accurate model. In \n",
    "such cases the model fits the noise in the data rather than fitting the relationship better, which leads to poor prediction on \n",
    "test/unseen data. Inflexible methods have lower risks of overfitting and hence would perform better in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) The relationship between the predictors and response is highly non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a flexible statistical learning method would have better performance than an inflexible one. The flexible methods\n",
    "are proved to be better at modeling non-linear, complex relationships between predictors and response than inflexible ones.\n",
    "Whereas, the inflexible methods assume the reltionships to be simple or linear and hence do not perform well when it comes to \n",
    "complex or non-linear relationships. Flexbile statistical learning methods such as Decision Trees and Non-linear regression \n",
    "are used to model such scenarios for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) The variance of the error terms, i.e. $σ^2$ = Var(ε), is extremely high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the variance of the error terms is too high, it shows that there is a lot of noise in the data which inevitably makes it \n",
    "difficult to accurately model the relationship between the predictors and response. As we discussed in the above questions \n",
    "flexible models run a higher risk of overfitting than inflexible ones. So if flexible methods are used in this case, it might \n",
    "try to fit the noise in the data leading to overfitting. Wheras, an inflexible method are less prone to overfitting and hence \n",
    "they would perform better than flexible method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. ISLR: 2.4.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_3d(point1, point2):\n",
    "    x1, y1, z1 = point1\n",
    "    x2, y2, z2 = point2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2 + (z2 - z1)**2)\n",
    "\n",
    "point1 = (0, 0, 0)\n",
    "#Observations from the questions\n",
    "points = [(0, 3, 0), (2,0,0), (0,1,3), (0,1,2),(-1,0,1), (1,1,1)]\n",
    "\n",
    "for point in points:\n",
    "    distance = euclidean_distance_3d(point1, point)\n",
    "    print('Euclidean Distance between', point,'& ', point1,' = ', distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) What is our prediction with K = 1? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When k=1, we are just finding the single nearest neighbor to our test point (0,0,0). And as the observation no.5 \n",
    "which is (-1,0,1) is closest to our test point with a minimum distance of 1.414. So our predicted Y will be same as the Y\n",
    "of closest point which is \"Green\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) What is our prediction with K = 3? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When k=3, we are finding three nearest neighbor to our test point (0,0,0) which will include observation no.5, observation no.6\n",
    "and observation no. 2 as they are the closest 3 points to our test point. So our predicted Y will be same as the Y that the \n",
    "majority of the nearest neighbors have and here two out of three neighbors have the Y = 'Red'. Hence the prediction will be\n",
    "\"Red\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) If the Bayes decision boundary in this problem is highly non-linear, then would we expect the best value for K to be large or small? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Bayes decision boundary is highly non-linear, we would need a model that gives better performance in case of non-linearity.The flexible statistical learning methods are better suited for modelling non-linear and complex relationships. For KNN to be more flexible, it should have a smaller value of K. As we take the average or majority of the K points, large K will lead to inflexible models and hence would not model the non-linear relationships accurately/better than small k.\n",
    "Hence best k values should be small"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "8c153f0d8f4138a67421f73583b6872e057feb8264546c4d7fe6138347411381"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
